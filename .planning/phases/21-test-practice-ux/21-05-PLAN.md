---
phase: 21-test-practice-ux
plan: 05
type: execute
wave: 2
depends_on: ["21-02"]
files_modified:
  - src/components/interview/ExaminerCharacter.tsx
  - src/components/interview/ChatBubble.tsx
  - src/components/interview/TypingIndicator.tsx
  - src/hooks/useSpeechRecognition.ts
  - src/hooks/useSilenceDetection.ts
autonomous: true
user_setup:
  - service: react-speech-recognition
    why: "Speech recognition React wrapper"
    env_vars: []
    dashboard_config: []

must_haves:
  truths:
    - "Examiner character has idle breathing, speaking, and nod animations"
    - "Chat bubbles display examiner and user messages with correct alignment"
    - "Speech recognition hook wraps react-speech-recognition with clean API"
    - "Silence detection auto-stops recording after 2 seconds of silence"
  artifacts:
    - path: "src/components/interview/ExaminerCharacter.tsx"
      provides: "Professional SVG examiner with CSS animations"
      exports: ["ExaminerCharacter"]
    - path: "src/components/interview/ChatBubble.tsx"
      provides: "Chat message bubble with sender-based alignment"
      exports: ["ChatBubble"]
    - path: "src/components/interview/TypingIndicator.tsx"
      provides: "Three-dot typing animation for examiner"
      exports: ["TypingIndicator"]
    - path: "src/hooks/useSpeechRecognition.ts"
      provides: "React hook wrapping react-speech-recognition"
      exports: ["useInterviewSpeech"]
    - path: "src/hooks/useSilenceDetection.ts"
      provides: "Silence detection via AnalyserNode RMS"
      exports: ["useSilenceDetection"]
  key_links:
    - from: "src/hooks/useSpeechRecognition.ts"
      to: "react-speech-recognition"
      via: "npm dependency"
      pattern: "import.*react-speech-recognition"
---

<objective>
Create interview-specific components (examiner character, chat bubbles, typing indicator) and speech recognition hooks.

Purpose: These are the building blocks for the chat-style interview overhaul -- the visual character, message layout, and voice input capabilities.
Output: ExaminerCharacter.tsx, ChatBubble.tsx, TypingIndicator.tsx, useSpeechRecognition.ts, useSilenceDetection.ts
</objective>

<execution_context>
@C:/Users/minkk/.claude/get-shit-done/workflows/execute-plan.md
@C:/Users/minkk/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/phases/21-test-practice-ux/21-CONTEXT.md
@.planning/phases/21-test-practice-ux/21-RESEARCH.md
@.planning/phases/21-test-practice-ux/21-02-SUMMARY.md

@src/components/interview/InterviewerAvatar.tsx
@src/components/interview/AudioWaveform.tsx
@src/hooks/useAudioRecorder.ts
@src/lib/motion-config.ts
</context>

<tasks>

<task type="auto">
  <name>Task 1: Examiner character, chat bubbles, and typing indicator</name>
  <files>src/components/interview/ExaminerCharacter.tsx, src/components/interview/ChatBubble.tsx, src/components/interview/TypingIndicator.tsx</files>
  <action>
**First:** Install react-speech-recognition: `pnpm add react-speech-recognition && pnpm add -D @types/react-speech-recognition`

**ExaminerCharacter.tsx** - Replace existing simple InterviewerAvatar with professional SVG character:

Props:
```typescript
interface ExaminerCharacterProps {
  state: 'idle' | 'speaking' | 'nodding' | 'listening';
  size?: 'sm' | 'md' | 'lg';  // sm=80px, md=120px, lg=160px
}
```

Per locked decisions:
- Professional illustration style (NOT cartoon) -- a suited figure with head, shoulders, tie, simple facial features (eyes, mouth line)
- Inline SVG with separate elements for animation targets:
  - `<g id="head">` -- for nod animation
  - `<g id="chest">` -- for breathing animation
  - `<g id="mouth">` -- for speaking animation
- CSS keyframe animations (NOT Lottie -- saves 133KB per research):
  - `idle`: Subtle breathing on chest group (scaleY oscillation 1 -> 1.02, 3s loop)
  - `speaking`: Mouth area scales (scaleY 1 -> 1.1 -> 0.9, 0.4s loop) + subtle chest movement
  - `nodding`: Head rotates slightly (-3deg -> 2deg, 0.6s, no loop -- fires once)
  - `listening`: Subtle head tilt + slow breathing (attentive posture)
- Character positioned at top of screen (large, prominent) per locked decision
- Professional style: suit/blazer, neutral colors, clean lines, friendly but formal expression
- Respect reduced motion: static pose when prefers-reduced-motion
- Use design tokens for colors (primary-700 for suit, primary-500 for skin tone placeholder)
- Add USCIS badge/logo accent for authenticity

**ChatBubble.tsx**:

Props:
```typescript
interface ChatBubbleProps {
  sender: 'examiner' | 'user';
  children: React.ReactNode;
  isCorrect?: boolean;        // For results transcript coloring
  confidence?: number;         // 0-1 for match quality indicator
  avatar?: React.ReactNode;    // Examiner avatar or user initials
  timestamp?: string;
}
```

Per locked decisions:
- Examiner messages: left-aligned, muted background (`bg-muted/40`), rounded with `rounded-tl-sm`
- User answers: right-aligned, primary background (`bg-primary text-white`), rounded with `rounded-tr-sm`
- `max-w-[80%]` to leave visual breathing room
- Motion entrance: `initial={{ opacity: 0, y: 8, scale: 0.95 }}`, SPRING_GENTLE
- In results: color border green/red based on isCorrect + Check/X icon
- Confidence indicator: small badge showing "85% match" when confidence prop provided
- Avatar slot to left of examiner bubbles, right of user bubbles

**TypingIndicator.tsx**:

Three bouncing dots in a chat bubble shape. Uses CSS animation for the sequential dot bounce pattern (dot 1 bounces, then dot 2, then dot 3, repeat). Matches examiner bubble styling (left-aligned, muted bg). Small component -- should be <30 lines.
  </action>
  <verify>Run `npx tsc --noEmit` and `npm run lint`. Verify SVG character renders without errors. Check CSS animations are defined inline or in component.</verify>
  <done>ExaminerCharacter renders professional SVG with idle/speaking/nodding/listening animations. ChatBubble supports examiner/user alignment with confidence badges. TypingIndicator shows animated dots.</done>
</task>

<task type="auto">
  <name>Task 2: Speech recognition and silence detection hooks</name>
  <files>src/hooks/useSpeechRecognition.ts, src/hooks/useSilenceDetection.ts</files>
  <action>
**useSpeechRecognition.ts** (wrapper around react-speech-recognition):

```typescript
interface UseInterviewSpeechReturn {
  transcript: string;
  isListening: boolean;
  isSupported: boolean;
  startListening: () => Promise<void>;
  stopListening: () => void;
  resetTranscript: () => void;
  error: string | null;
}

export function useInterviewSpeech(): UseInterviewSpeechReturn
```

Implementation:
- Import from `react-speech-recognition` (SpeechRecognition default + useSpeechRecognition hook)
- `startListening()`: resets transcript, calls `SpeechRecognition.startListening({ continuous: false, language: 'en-US' })`
- `stopListening()`: calls `SpeechRecognition.stopListening()`
- `isSupported`: maps from `browserSupportsSpeechRecognition`
- Error handling: if not supported, set error = "Speech recognition is not supported in this browser. Please use Chrome for the best experience."
- Safari Siri check: Show messaging "Speech recognition requires Siri to be enabled on Apple devices." when on Safari and recognition fails
- Per research pitfall: speech recognition requires HTTPS (localhost exempted). Add check.
- When not supported, return { isSupported: false } -- caller will show self-grade buttons as fallback

**useSilenceDetection.ts**:

```typescript
interface UseSilenceDetectionOptions {
  stream: MediaStream | null;
  silenceThreshold?: number;   // Default 0.01
  silenceMs?: number;          // Default 2000 (2 seconds)
  onSilence: () => void;
  enabled?: boolean;           // Toggle on/off
}

export function useSilenceDetection(options: UseSilenceDetectionOptions): void
```

Implementation per research code example:
- Create AudioContext and AnalyserNode from stream
- Use `requestAnimationFrame` loop to check RMS level
- If RMS < threshold for silenceMs duration, call onSilence()
- Cleanup: cancelAnimationFrame, disconnect source, close AudioContext
- Only run when `enabled` is true and stream is not null
- Use `useEffect` with proper cleanup (cancel flag pattern for React Compiler)
- Per pitfall: don't create AudioContext at module level -- create inside effect

Important: These hooks must NOT overlap with TTS (SpeechSynthesis). The interview orchestrator (plan 08) will sequence: TTS finishes -> start recording. These hooks just provide the capability.
  </action>
  <verify>Run `npx tsc --noEmit` and `npm run lint`. Verify react-speech-recognition is in package.json. Verify types import from @types/react-speech-recognition.</verify>
  <done>useInterviewSpeech wraps react-speech-recognition with clean API and browser support detection. useSilenceDetection auto-detects silence from microphone stream after 2 seconds.</done>
</task>

</tasks>

<verification>
- `npx tsc --noEmit` passes
- `npm run lint` passes
- `pnpm ls react-speech-recognition` shows installed
- ExaminerCharacter has 4 animation states
- ChatBubble supports examiner/user alignment
- Speech recognition hook detects browser support
</verification>

<success_criteria>
- Professional SVG examiner character with CSS keyframe animations (no Lottie)
- Chat bubble layout matches Duolingo/iMessage conversation pattern
- Speech recognition hook provides clean API with browser fallback
- Silence detection works with 2-second threshold
</success_criteria>

<output>
After completion, create `.planning/phases/21-test-practice-ux/21-05-SUMMARY.md`
</output>
